{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Merlin XGBoost\n",
    "Complete implementation with proper memory management and debugging outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudf-cu11                 23.8.0\n",
      "cupy-cuda11x              13.6.0\n",
      "dask-cuda                 23.8.0\n",
      "dask-cudf-cu11            23.8.0\n",
      "npy-append-array          0.9.19\n",
      "numpy                     1.24.3\n",
      "nvtabular                 23.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E 'numpy|nvtabular|cudf|cupy|npy-append-array|dask-cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ nvtabular       23.08.00        (required: ‚â•23.08.00)\n",
      "‚úÖ cudf            23.08.00        (required: ‚â•23.10)\n",
      "‚úÖ cupy            13.6.0          (required: ‚â•13.6)\n",
      "‚úÖ xgboost         3.0.5           (required: ‚â•3.0)\n",
      "‚úÖ dask            2023.9.2        (required: ‚â•2023.9)\n",
      "‚úÖ pandas          1.5.3           (required: ‚â•1.5)\n",
      "‚úÖ numpy           1.24.3          (required: ‚â•1.24)\n",
      "‚úÖ scikit-learn    1.7.1           (required: ‚â•1.7)\n",
      "‚úÖ psutil          5.9.1           (required: ‚â•5.9)\n",
      "‚úÖ pyarrow         12.0.1          (required: ‚â•12.0)\n",
      "\n",
      "‚úÖ All required libraries are installed and compatible!\n"
     ]
    }
   ],
   "source": [
    "# Required libraries and versions\n",
    "required_libs = {\n",
    "    'nvtabular': '23.08.00',\n",
    "    'cudf': '23.10',      # Prefix match\n",
    "    'cupy': '13.6',       # Prefix match  \n",
    "    'xgboost': '3.0',     # Minimum version\n",
    "    'dask': '2023.9',\n",
    "    'pandas': '1.5',\n",
    "    'numpy': '1.24',      # 1.24.3 is working\n",
    "    'scikit-learn': '1.7',\n",
    "    'psutil': '5.9',      # 5.9.1 works fine (used in working code)\n",
    "    'pyarrow': '12.0'     # 12.0.1 works fine (used in working code)\n",
    "}\n",
    "\n",
    "# Check installed versions\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    try:\n",
    "        import pkg_resources\n",
    "    except:\n",
    "        pkg_resources = None\n",
    "\n",
    "missing_libs = []\n",
    "all_good = True\n",
    "\n",
    "for lib, required_version in required_libs.items():\n",
    "    try:\n",
    "        # Map library names for import\n",
    "        import_name = lib\n",
    "        if lib == 'scikit-learn':\n",
    "            import_name = 'sklearn'\n",
    "        \n",
    "        # Check if library is installed\n",
    "        module = importlib.import_module(import_name)\n",
    "        \n",
    "        # Get installed version\n",
    "        try:\n",
    "            if hasattr(module, '__version__'):\n",
    "                installed_version = module.__version__\n",
    "            elif pkg_resources:\n",
    "                installed_version = pkg_resources.get_distribution(lib).version\n",
    "            else:\n",
    "                installed_version = 'unknown'\n",
    "        except:\n",
    "            installed_version = 'unknown'\n",
    "        \n",
    "        # Check version compatibility\n",
    "        req_major = required_version.split('.')[0]\n",
    "        inst_version_parts = installed_version.split('.')\n",
    "        inst_major = inst_version_parts[0] if installed_version != 'unknown' else ''\n",
    "        \n",
    "        # More lenient version check\n",
    "        if installed_version == 'unknown':\n",
    "            print(f\"‚ö†Ô∏è  {lib:15} {installed_version:15} (required: ‚â•{required_version})\")\n",
    "        elif float(inst_major) >= float(req_major) if inst_major.isdigit() and req_major.isdigit() else installed_version.startswith(required_version[:3]):\n",
    "            print(f\"‚úÖ {lib:15} {installed_version:15} (required: ‚â•{required_version})\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {lib:15} {installed_version:15} (required: ‚â•{required_version}) - but should work\")\n",
    "        \n",
    "    except ImportError:\n",
    "        missing_libs.append(lib)\n",
    "        print(f\"‚ùå {lib:15} NOT INSTALLED (required: ‚â•{required_version})\")\n",
    "        all_good = False\n",
    "\n",
    "# Report\n",
    "if missing_libs:\n",
    "    print(f\"\\n‚ùå Missing libraries: {', '.join(missing_libs)}\")\n",
    "    print(\"Please install them using conda or pip\")\n",
    "elif all_good:\n",
    "    print(\"\\n‚úÖ All required libraries are installed and compatible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "NVTabular version: 23.08.00\n",
      "XGBoost version: 3.0.5\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "\n",
    "# GPU libraries\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "from nvtabular import ops\n",
    "from merlin.io import Dataset\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Model save\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"NVTabular version: {nvt.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Current PATH: /workspace\n",
      "üìã Configuration:\n",
      "   Input: train.parquet\n",
      "   Output: /output\n",
      "   Folds: 5\n",
      "   Force reprocess: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration(DATA PATH)\n",
    "TRAIN_PATH = 'train.parquet'\n",
    "OUTPUT_DIR = '/output'  # Í≤ΩÎ°úÎäî\n",
    "TEMP_DIR = '/workspace/tmp'       # Ïù¥Í≤å ÎßûÎäî Í≤É Í∞ôÏùå\n",
    "N_FOLDS = 5\n",
    "FORCE_REPROCESS = False  # Set to True to reprocess data\n",
    "\n",
    "print(f\"üìã Current PATH: {os.getcwd()}\")\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Input: {TRAIN_PATH}\")\n",
    "print(f\"   Output: {OUTPUT_DIR}\")\n",
    "print(f\"   Folds: {N_FOLDS}\")\n",
    "print(f\"   Force reprocess: {FORCE_REPROCESS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory functions:\n",
      "üíæ CPU: 33.1GB/251.5GB (14.0%)\n",
      "üíæ GPU: 0.9GB/48.0GB\n",
      "üßπ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "# Memory management functions\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_used = gpu_info.used / 1024**3\n",
    "        gpu_total = gpu_info.total / 1024**3\n",
    "    except:\n",
    "        gpu_used = 0\n",
    "        gpu_total = 0\n",
    "    \n",
    "    print(f\"üíæ CPU: {mem.used/1024**3:.1f}GB/{mem.total/1024**3:.1f}GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"üíæ GPU: {gpu_used:.1f}GB/{gpu_total:.1f}GB\")\n",
    "    return mem.percent\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "    print(\"üßπ GPU memory cleared\")\n",
    "\n",
    "# Test memory functions\n",
    "print(\"Testing memory functions:\")\n",
    "print_memory()\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metric functions defined\n"
     ]
    }
   ],
   "source": [
    "# Metric functions\n",
    "def calculate_weighted_logloss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Calculate Weighted LogLoss with 50:50 class weights\"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    mask_0 = (y_true == 0)\n",
    "    mask_1 = (y_true == 1)\n",
    "    \n",
    "    ll_0 = -np.mean(np.log(1 - y_pred[mask_0])) if mask_0.sum() > 0 else 0\n",
    "    ll_1 = -np.mean(np.log(y_pred[mask_1])) if mask_1.sum() > 0 else 0\n",
    "    \n",
    "    return 0.5 * ll_0 + 0.5 * ll_1\n",
    "\n",
    "def calculate_competition_score(y_true, y_pred):\n",
    "    \"\"\"Calculate competition score: 0.5*AP + 0.5*(1/(1+WLL))\"\"\"\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    wll = calculate_weighted_logloss(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "    return score, ap, wll\n",
    "\n",
    "print(\"‚úÖ Metric functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   ‚úÖ Workflow created (no normalization for tree models)\n",
      "‚úÖ Workflow creation tested successfully\n"
     ]
    }
   ],
   "source": [
    "def create_workflow():\n",
    "    \"\"\"Create NVTabular workflow optimized for XGBoost\"\"\"\n",
    "    print(\"\\nüîß Creating XGBoost-optimized workflow...\")\n",
    "    \n",
    "    # TRUE CATEGORICAL COLUMNS (only 5)\n",
    "    true_categorical = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    \n",
    "    # CONTINUOUS COLUMNS (112 total)\n",
    "    all_continuous = (\n",
    "        [f'feat_a_{i}' for i in range(1, 19)] +  # 18\n",
    "        [f'feat_b_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_c_{i}' for i in range(1, 9)] +   # 8\n",
    "        [f'feat_d_{i}' for i in range(1, 7)] +   # 6\n",
    "        [f'feat_e_{i}' for i in range(1, 11)] +  # 10\n",
    "        [f'history_a_{i}' for i in range(1, 8)] +  # 7\n",
    "        [f'history_b_{i}' for i in range(1, 31)] + # 30\n",
    "        [f'l_feat_{i}' for i in range(1, 28)]      # 27\n",
    "    )\n",
    "    \n",
    "    print(f\"   Categorical: {len(true_categorical)} columns\")\n",
    "    print(f\"   Continuous: {len(all_continuous)} columns\")\n",
    "    print(f\"   Total features: {len(true_categorical) + len(all_continuous)}\")\n",
    "    \n",
    "    # Minimal preprocessing for XGBoost\n",
    "    cat_features = true_categorical >> ops.Categorify(\n",
    "        freq_threshold=0,   # 10Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨ \n",
    "        # cat_cache=\"device\"     # ‚úÖ GPU‚ÜíCPU dictionary Ï†ÄÏû•\n",
    "        max_size=50000\n",
    "    )\n",
    "    cont_features = all_continuous >> ops.FillMissing(fill_val=0)\n",
    "    \n",
    "    workflow = nvt.Workflow(cat_features + cont_features + ['clicked'])\n",
    "    \n",
    "    print(\"   ‚úÖ Workflow created (no normalization for tree models)\")  # ÎÇòÏò¥\n",
    "    return workflow\n",
    "\n",
    "# Test workflow creation\n",
    "test_workflow = create_workflow()\n",
    "print(\"‚úÖ Workflow creation tested successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ NVTabular Data Processing\n",
      "======================================================================\n",
      "üíæ CPU: 33.3GB/251.5GB (14.1%)\n",
      "üíæ GPU: 0.9GB/48.0GB\n",
      "üóëÔ∏è Removing existing temp directory: /workspace/tmp/train_no_seq\n",
      "\n",
      "üìã Creating temp file without 'seq' column...\n",
      "   Total columns: 119\n",
      "   Using columns: 118 (excluded 'seq')\n",
      "   Loaded 10,704,179 rows\n",
      "   Saved part_0.parquet\n",
      "   Saved part_1.parquet\n",
      "   Saved part_2.parquet\n",
      "   Saved part_3.parquet\n",
      "   Saved part_4.parquet\n",
      "   Saved part_5.parquet\n",
      "   Saved part_6.parquet\n",
      "   Saved part_7.parquet\n",
      "   Saved part_8.parquet\n",
      "   Saved part_9.parquet\n",
      "   ‚úÖ Temp files created\n",
      "\n",
      "üì¶ Creating NVTabular Dataset...\n",
      "   Using 32MB partitions for memory efficiency\n",
      "üßπ GPU memory cleared\n",
      "üìÅ TMP ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞:\n",
      "    /workspace/tmp/train_no_seq/part_6.parquet\n",
      "    /workspace/tmp/train_no_seq/part_4.parquet\n",
      "    /workspace/tmp/train_no_seq/part_5.parquet\n",
      "    /workspace/tmp/train_no_seq/part_7.parquet\n",
      "    /workspace/tmp/train_no_seq/part_0.parquet\n",
      "    /workspace/tmp/train_no_seq/part_9.parquet\n",
      "    /workspace/tmp/train_no_seq/part_3.parquet\n",
      "    /workspace/tmp/train_no_seq/part_2.parquet\n",
      "    /workspace/tmp/train_no_seq/part_1.parquet\n",
      "    /workspace/tmp/train_no_seq/part_8.parquet\n",
      "   ‚úÖ Dataset created\n",
      "\n",
      "üìä Fitting workflow...\n",
      "\n",
      "üîß Creating XGBoost-optimized workflow...\n",
      "   Categorical: 5 columns\n",
      "   Continuous: 112 columns\n",
      "   Total features: 117\n",
      "   ‚úÖ Workflow created (no normalization for tree models)\n",
      "   ‚úÖ Before fit\n",
      "   ‚úÖ Workflow fitted\n",
      "\n",
      "üíæ Transforming and saving to /output...\n",
      "   ‚úÖ Data processed and saved\n",
      "   ‚úÖ Workflow saved to /workspace/output/workflow\n",
      "üíæ CPU: 40.4GB/251.5GB (16.9%)\n",
      "üíæ GPU: 0.9GB/48.0GB\n",
      "\n",
      "‚úÖ Processing complete!\n",
      "   Time: 316.9s\n",
      "   Memory increase: +2.8%\n",
      "üßπ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "def process_data():\n",
    "    \"\"\"Process data with NVTabular\"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ NVTabular Data Processing\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if already processed\n",
    "    if os.path.exists(OUTPUT_DIR) and not FORCE_REPROCESS:\n",
    "        try:\n",
    "            test_dataset = Dataset(OUTPUT_DIR, engine='parquet')\n",
    "            print(f\"‚úÖ Using existing processed data from {OUTPUT_DIR}\")\n",
    "            return OUTPUT_DIR\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Existing data corrupted, reprocessing...\")\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    # Clear existing if needed\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        print(f\"üóëÔ∏è Removing existing directory {OUTPUT_DIR}\")\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    initial_mem = print_memory()\n",
    "    \n",
    "    temp_path = f'{TEMP_DIR}/train_no_seq'  # ÌôïÏû•Ïûê Ï†úÍ±∞\n",
    "    \n",
    "    # üîπ Ìï≠ÏÉÅ Í∏∞Ï°¥ ÌååÏùº/ÎîîÎ†âÌÜ†Î¶¨Î•º ÏïàÏ†ÑÌïòÍ≤å Ï†ïÎ¶¨\n",
    "    if os.path.exists(temp_path):\n",
    "        print(f\"üóëÔ∏è Removing existing temp directory: {temp_path}\")\n",
    "        shutil.rmtree(temp_path)\n",
    "    \n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nüìã Creating temp file without 'seq' column...\")\n",
    "    pf = pq.ParquetFile(TRAIN_PATH)\n",
    "    cols = [c for c in pf.schema.names if c != 'seq']\n",
    "    print(f\"   Total columns: {len(pf.schema.names)}\")\n",
    "    print(f\"   Using columns: {len(cols)} (excluded 'seq')\")\n",
    "    \n",
    "    df = pd.read_parquet(TRAIN_PATH, columns=cols)\n",
    "    print(f\"   Loaded {len(df):,} rows\")\n",
    "    \n",
    "    # ‚úÖ Ïó¨Îü¨ Í∞ú ÌååÏùºÎ°ú Î∂ÑÌï† Ï†ÄÏû•\n",
    "    n_splits = 10\n",
    "    split_size = len(df) // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start, end = i * split_size, (i + 1) * split_size\n",
    "        df.iloc[start:end].to_parquet(f\"{temp_path}/part_{i}.parquet\", index=False)\n",
    "        print(f\"   Saved part_{i}.parquet\")\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    print(\"   ‚úÖ Temp files created\")\n",
    "    \n",
    "    # Create dataset with small partitions\n",
    "    print(\"\\nüì¶ Creating NVTabular Dataset...\")\n",
    "    print(\"   Using 32MB partitions for memory efficiency\")\n",
    "    clear_gpu_memory()  # Ïó¨Í∏∞ÍπåÏßÑ Í∞îÏùå\n",
    "\n",
    "    print(\"üìÅ TMP ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞:\")  # ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ ÌôïÏù∏\n",
    "    for root, dirs, files in os.walk(TEMP_DIR):\n",
    "        for f in files:\n",
    "            print(\"   \", os.path.join(root, f))\n",
    "    \n",
    "    dataset = Dataset(\n",
    "        temp_path,\n",
    "        engine='parquet',\n",
    "        part_size='8MB',  #change size based on your environment: 32MB -> 8MB\n",
    "        cpu = True  # cpu Í∏∞Î∞òÏúºÎ°ú ÏùΩÍ∏∞. gpu Î©îÎ™®Î¶¨Î•º Ïïà Ïç®ÏÑú ÏÜçÎèÑ Îñ®Ïñ¥Ïßê\n",
    "    )\n",
    "    print(\"   ‚úÖ Dataset created\")  # ÎÇòÏò¥\n",
    "    \n",
    "    # Create and fit workflow\n",
    "    print(\"\\nüìä Fitting workflow...\")\n",
    "    workflow = create_workflow()  \n",
    "    print(\"   ‚úÖ Before fit\")\n",
    "    workflow.fit(dataset)  # ÏïÑÎßà Ïù¥ Ìï®Ïàò Ïã§Ìñâ Ï§ëÏóê Ï£ΩÏùå?\n",
    "    print(\"   ‚úÖ Workflow fitted\")\n",
    "    \n",
    "    # Transform and save\n",
    "    print(f\"\\nüíæ Transforming and saving to {OUTPUT_DIR}...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)  # Ìè¥Îçî ÏÉùÏÑ±Îßå Ìï®\n",
    "\n",
    "    try:\n",
    "        workflow.transform(dataset).to_parquet(\n",
    "            output_path=OUTPUT_DIR,\n",
    "            shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "            out_files_per_proc=8\n",
    "        )\n",
    "        \n",
    "        workflow_path = f'/workspace{OUTPUT_DIR}/workflow'\n",
    "        # workflow.save(workflow_path)  # Ï†ÑÏ≤òÎ¶¨ Í∑úÏπô Ï†ÄÏû•!\n",
    "        print(f\"   ‚úÖ Data processed and saved\")\n",
    "        print(f\"   ‚úÖ Workflow saved to {workflow_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {e}\")\n",
    "        if os.path.exists(OUTPUT_DIR):\n",
    "            shutil.rmtree(OUTPUT_DIR)\n",
    "        raise\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    final_mem = print_memory()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processing complete!\")\n",
    "    print(f\"   Time: {elapsed:.1f}s\")\n",
    "    print(f\"   Memory increase: +{final_mem - initial_mem:.1f}%\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    return OUTPUT_DIR\n",
    "\n",
    "# Process data\n",
    "processed_dir = process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîÑ Stratified KFold Cross-Validation (Sampled & Lazy)\n",
      "======================================================================\n",
      "\n",
      "üì¶ Full rows from dataset...\n",
      "‚úÖ Sample loaded: 10,704,170 rows\n",
      "üíæ CPU: 27.8GB/251.5GB (11.9%)\n",
      "üíæ GPU: 0.9GB/48.0GB\n",
      "\n",
      "üìä Preparing data for XGBoost...\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìä Class distribution:\n",
      "   Positive ratio: 0.0191\n",
      "   Scale pos weight: 51.43\n",
      "\n",
      "üîÑ Starting cross-validation...\n",
      "\n",
      "üìç Fold 1/5\n",
      "   Train: 8,563,336 | Val: 2,140,834\n",
      "   Training...\n",
      "[0]\tval-logloss:0.67958\n",
      "[10]\tval-logloss:0.61926\n",
      "[20]\tval-logloss:0.60230\n",
      "[30]\tval-logloss:0.59386\n",
      "[40]\tval-logloss:0.58859\n",
      "[50]\tval-logloss:0.58446\n",
      "[60]\tval-logloss:0.58188\n",
      "[70]\tval-logloss:0.57941\n",
      "[80]\tval-logloss:0.57716\n",
      "[90]\tval-logloss:0.57559\n",
      "[100]\tval-logloss:0.57389\n",
      "[110]\tval-logloss:0.57189\n",
      "[120]\tval-logloss:0.57005\n",
      "[130]\tval-logloss:0.56810\n",
      "[140]\tval-logloss:0.56627\n",
      "[150]\tval-logloss:0.56470\n",
      "[160]\tval-logloss:0.56348\n",
      "[170]\tval-logloss:0.56186\n",
      "[180]\tval-logloss:0.56041\n",
      "[190]\tval-logloss:0.55869\n",
      "[199]\tval-logloss:0.55735\n",
      "   üìä Results:\n",
      "      Score: 0.352553\n",
      "      AP: 0.080276\n",
      "      WLL: 0.600436\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 38.0s\n",
      "üíæ Model saved to xgb_model_fold1.pkl\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 2/5\n",
      "   Train: 8,563,336 | Val: 2,140,834\n",
      "   Training...\n",
      "[0]\tval-logloss:0.67951\n",
      "[10]\tval-logloss:0.62010\n",
      "[20]\tval-logloss:0.60350\n",
      "[30]\tval-logloss:0.59510\n",
      "[40]\tval-logloss:0.58916\n",
      "[50]\tval-logloss:0.58559\n",
      "[60]\tval-logloss:0.58271\n",
      "[70]\tval-logloss:0.58030\n",
      "[80]\tval-logloss:0.57761\n",
      "[90]\tval-logloss:0.57575\n",
      "[100]\tval-logloss:0.57420\n",
      "[110]\tval-logloss:0.57195\n",
      "[120]\tval-logloss:0.57032\n",
      "[130]\tval-logloss:0.56834\n",
      "[140]\tval-logloss:0.56652\n",
      "[150]\tval-logloss:0.56521\n",
      "[160]\tval-logloss:0.56347\n",
      "[170]\tval-logloss:0.56190\n",
      "[180]\tval-logloss:0.56013\n",
      "[190]\tval-logloss:0.55837\n",
      "[199]\tval-logloss:0.55715\n",
      "   üìä Results:\n",
      "      Score: 0.352679\n",
      "      AP: 0.080711\n",
      "      WLL: 0.600904\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 32.2s\n",
      "üíæ Model saved to xgb_model_fold2.pkl\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 3/5\n",
      "   Train: 8,563,336 | Val: 2,140,834\n",
      "   Training...\n",
      "[0]\tval-logloss:0.67946\n",
      "[10]\tval-logloss:0.61986\n",
      "[20]\tval-logloss:0.60296\n",
      "[30]\tval-logloss:0.59432\n",
      "[40]\tval-logloss:0.58839\n",
      "[50]\tval-logloss:0.58469\n",
      "[60]\tval-logloss:0.58215\n",
      "[70]\tval-logloss:0.57924\n",
      "[80]\tval-logloss:0.57673\n",
      "[90]\tval-logloss:0.57503\n",
      "[100]\tval-logloss:0.57313\n",
      "[110]\tval-logloss:0.57163\n",
      "[120]\tval-logloss:0.56994\n",
      "[130]\tval-logloss:0.56812\n",
      "[140]\tval-logloss:0.56612\n",
      "[150]\tval-logloss:0.56454\n",
      "[160]\tval-logloss:0.56281\n",
      "[170]\tval-logloss:0.56116\n",
      "[180]\tval-logloss:0.55974\n",
      "[190]\tval-logloss:0.55820\n",
      "[199]\tval-logloss:0.55708\n",
      "   üìä Results:\n",
      "      Score: 0.352963\n",
      "      AP: 0.080528\n",
      "      WLL: 0.598979\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 31.5s\n",
      "üíæ Model saved to xgb_model_fold3.pkl\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 4/5\n",
      "   Train: 8,563,336 | Val: 2,140,834\n",
      "   Training...\n",
      "[0]\tval-logloss:0.67966\n",
      "[10]\tval-logloss:0.61994\n",
      "[20]\tval-logloss:0.60353\n",
      "[30]\tval-logloss:0.59492\n",
      "[40]\tval-logloss:0.58921\n",
      "[50]\tval-logloss:0.58581\n",
      "[60]\tval-logloss:0.58264\n",
      "[70]\tval-logloss:0.58041\n",
      "[80]\tval-logloss:0.57785\n",
      "[90]\tval-logloss:0.57539\n",
      "[100]\tval-logloss:0.57388\n",
      "[110]\tval-logloss:0.57199\n",
      "[120]\tval-logloss:0.57011\n",
      "[130]\tval-logloss:0.56828\n",
      "[140]\tval-logloss:0.56669\n",
      "[150]\tval-logloss:0.56487\n",
      "[160]\tval-logloss:0.56331\n",
      "[170]\tval-logloss:0.56161\n",
      "[180]\tval-logloss:0.56046\n",
      "[190]\tval-logloss:0.55891\n",
      "[199]\tval-logloss:0.55727\n",
      "   üìä Results:\n",
      "      Score: 0.353225\n",
      "      AP: 0.080902\n",
      "      WLL: 0.598596\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 32.4s\n",
      "üíæ Model saved to xgb_model_fold4.pkl\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üìç Fold 5/5\n",
      "   Train: 8,563,336 | Val: 2,140,834\n",
      "   Training...\n",
      "[0]\tval-logloss:0.67972\n",
      "[10]\tval-logloss:0.62038\n",
      "[20]\tval-logloss:0.60315\n",
      "[30]\tval-logloss:0.59471\n",
      "[40]\tval-logloss:0.59020\n",
      "[50]\tval-logloss:0.58560\n",
      "[60]\tval-logloss:0.58273\n",
      "[70]\tval-logloss:0.58021\n",
      "[80]\tval-logloss:0.57825\n",
      "[90]\tval-logloss:0.57576\n",
      "[100]\tval-logloss:0.57393\n",
      "[110]\tval-logloss:0.57217\n",
      "[120]\tval-logloss:0.57030\n",
      "[130]\tval-logloss:0.56860\n",
      "[140]\tval-logloss:0.56684\n",
      "[150]\tval-logloss:0.56547\n",
      "[160]\tval-logloss:0.56367\n",
      "[170]\tval-logloss:0.56212\n",
      "[180]\tval-logloss:0.56067\n",
      "[190]\tval-logloss:0.55893\n",
      "[199]\tval-logloss:0.55762\n",
      "   üìä Results:\n",
      "      Score: 0.353915\n",
      "      AP: 0.082238\n",
      "      WLL: 0.598484\n",
      "      Best iteration: 199\n",
      "   ‚è±Ô∏è Time: 32.0s\n",
      "üíæ Model saved to xgb_model_fold5.pkl\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "======================================================================\n",
      "üìä Final Cross-Validation Results\n",
      "======================================================================\n",
      "\n",
      "üèÜ Competition Score: 0.353067 ¬± 0.000484\n",
      "üìà Average Precision: 0.080931 ¬± 0.000685\n",
      "üìâ Weighted LogLoss: 0.599480 ¬± 0.000996\n",
      "\n",
      "All fold scores: ['0.352553', '0.352679', '0.352963', '0.353225', '0.353915']\n"
     ]
    }
   ],
   "source": [
    "def run_cv_optimized(processed_dir, n_folds=5):\n",
    "    \"\"\"Run stratified cross-validation using sampled data to prevent GPU OOM\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîÑ Stratified KFold Cross-Validation (Sampled & Lazy)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = Dataset(processed_dir, engine='parquet', part_size='32MB', cpu=True)\n",
    "\n",
    "    print(f\"\\nüì¶ Full rows from dataset...\")\n",
    "    # Lazy\n",
    "    gdf = dataset.to_ddf()\n",
    "    # Sampling & Early\n",
    "    # gdf = dataset.to_ddf().sample(frac=1.0, random_state=42).compute()\n",
    "\n",
    "    print(f\"‚úÖ Sample loaded: {len(gdf):,} rows\")\n",
    "    print_memory()\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nüìä Preparing data for XGBoost...\")\n",
    "\n",
    "    # Feature / Label split\n",
    "    feature_cols = [col for col in gdf.columns if col != \"clicked\"]\n",
    "    X = gdf[feature_cols]  # Already DataFrame (cpu load)\n",
    "    y = gdf[\"clicked\"]\n",
    "\n",
    "    del gdf\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "\n",
    "    # Class distribution\n",
    "    pos_ratio = y.mean()\n",
    "    scale_pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "    print(f\"\\nüìä Class distribution:\")\n",
    "    print(f\"   Positive ratio: {pos_ratio:.4f}\")\n",
    "    print(f\"   Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "    # XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'gpu_id': 0,\n",
    "        'verbosity': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # Stratified CV\n",
    "    print(\"\\nüîÑ Starting cross-validation...\")\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    cv_ap = []\n",
    "    cv_wll = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\nüìç Fold {fold}/{n_folds}\")\n",
    "        fold_start = time.time()\n",
    "\n",
    "        # Create DMatrix\n",
    "        print(f\"   Train: {len(train_idx):,} | Val: {len(val_idx):,}\")\n",
    "        dtrain = xgb.DMatrix(X[train_idx], label=y[train_idx])\n",
    "        dval = xgb.DMatrix(X[val_idx], label=y[val_idx])\n",
    "\n",
    "        # Train\n",
    "        print(\"   Training...\")\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=200,\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=10  # or False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(dval)\n",
    "        score, ap, wll = calculate_competition_score(y[val_idx], y_pred)\n",
    "        \n",
    "        cv_scores.append(score)\n",
    "        cv_ap.append(ap)\n",
    "        cv_wll.append(wll)\n",
    "        \n",
    "        print(f\"   üìä Results:\")\n",
    "        print(f\"      Score: {score:.6f}\")\n",
    "        print(f\"      AP: {ap:.6f}\")\n",
    "        print(f\"      WLL: {wll:.6f}\")\n",
    "        print(f\"      Best iteration: {model.best_iteration}\")\n",
    "        print(f\"   ‚è±Ô∏è Time: {time.time() - fold_start:.1f}s\")\n",
    "\n",
    "        # ‚úÖ Îß§ fold Î™®Îç∏ Ï†ÄÏû•\n",
    "        # joblib.dump(model, f\"xgb_model_fold{fold}.pkl\")\n",
    "        print(f\"üíæ Model saved to xgb_model_fold{fold}.pkl\")\n",
    "        \n",
    "        # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨ Cleanup\n",
    "        del dtrain, dval, model\n",
    "        clear_gpu_memory()\n",
    "\n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä Final Cross-Validation Results\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüèÜ Competition Score: {np.mean(cv_scores):.6f} ¬± {np.std(cv_scores):.6f}\")\n",
    "    print(f\"üìà Average Precision: {np.mean(cv_ap):.6f} ¬± {np.std(cv_ap):.6f}\")\n",
    "    print(f\"üìâ Weighted LogLoss: {np.mean(cv_wll):.6f} ¬± {np.std(cv_wll):.6f}\")\n",
    "    \n",
    "    print(f\"\\nAll fold scores: {[f'{s:.6f}' for s in cv_scores]}\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Run cross-validation\n",
    "cv_scores = run_cv_optimized(processed_dir, N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n",
      "COMPLETE!\n",
      "üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n",
      "\n",
      "‚úÖ Final CV Score: 0.353067 ¬± 0.000484\n",
      "‚úÖ Full dataset processed (10.7M rows)\n",
      "‚úÖ XGBoost-optimized preprocessing (no normalization)\n",
      "‚úÖ Memory-efficient with small partitions\n",
      "======================================================================\n",
      "üßπ GPU memory cleared\n",
      "\n",
      "üßπ Final cleanup complete\n",
      "üíæ CPU: 35.3GB/251.5GB (14.9%)\n",
      "üíæ GPU: 0.9GB/48.0GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final summary\n",
    "if cv_scores:\n",
    "    print(\"\\n\" + \"üéâ\"*35)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"üéâ\"*35)\n",
    "    print(f\"\\n‚úÖ Final CV Score: {np.mean(cv_scores):.6f} ¬± {np.std(cv_scores):.6f}\")\n",
    "    print(\"‚úÖ Full dataset processed (10.7M rows)\")\n",
    "    print(\"‚úÖ XGBoost-optimized preprocessing (no normalization)\")\n",
    "    print(\"‚úÖ Memory-efficient with small partitions\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Cross-validation did not complete. Please check for errors above.\")\n",
    "\n",
    "# Final cleanup\n",
    "clear_gpu_memory()\n",
    "print(\"\\nüßπ Final cleanup complete\")\n",
    "print_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
